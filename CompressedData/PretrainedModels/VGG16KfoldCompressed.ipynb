{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import PIL\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #allows us to augment and process data fed into a CNN\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os,cv2,random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #A tool that allows us to augment and process data to be fed into a CNN\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_inception = \"D:/amitavsir/bcd/data/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "local_weights_file = path_inception\n",
    "pre_trained_model = VGG16(input_shape = (121,121,3),\n",
    "              include_top = False,\n",
    "              weights = None)\n",
    "pre_trained_model.load_weights(local_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last layer output shape :  (?, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "#Locking the weights and parameters of pretrained model\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Getting desired layer output\n",
    "last_layer = pre_trained_model.get_layer('block5_conv2')\n",
    "last = last_layer.output\n",
    "print('Last layer output shape : ', last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "# Model configuration\n",
    "batch_size = 50\n",
    "img_size, img_num_channels = 121, 3\n",
    "loss_function = categorical_crossentropy\n",
    "no_classes = 4\n",
    "no_epochs = 20\n",
    "optimizer = Adam()\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='D:/amitavsir/bcd/datacompressed/train'\n",
    "list_folder=os.listdir(path = data_path)\n",
    "data=[]\n",
    "im_size=121    \n",
    "for i in list_folder:\n",
    "    new_path=os.path.join(data_path,i) \n",
    "    pic_list=os.listdir(new_path)                                               \n",
    "    for img in pic_list:\n",
    "        pic=os.path.join(new_path,img)   \n",
    "        arr=cv2.imread(pic)    \n",
    "        data.append([arr,list_folder.index(i)])    \n",
    "        \n",
    "random.shuffle(data)  \n",
    "x_train,y_train=[],[]\n",
    "for i,j in data:\n",
    "    x_train.append(i)\n",
    "    y_train.append(j)\n",
    "x_train=np.array(x_train).reshape(-1,im_size,im_size,img_num_channels)\n",
    "y_train=np.array(y_train).reshape(-1,1)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "X_train = x_train/255\n",
    "y_train = y_train.toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train,y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse numbers as floats\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = [] #stores accuracy\n",
    "loss_per_fold = [] #stores losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 962 images belonging to 4 classes.\n",
      "Found 500 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#class for training set\n",
    "train_data = ImageDataGenerator(rescale = 1./255, #pixel values rescaled so that it stays between 0 and 1.\n",
    "                                shear_range = 0.2, #this and the next 4 steps are used to augment our data by shearing it, flipping and zooming it to produce more examples per image.\n",
    "                                zoom_range = 0.2,  #Data augmentation allows us to prevent overfitting of data into the training set.\n",
    "                                horizontal_flip = True,\n",
    "                                vertical_flip = True,\n",
    "                                rotation_range = 40,\n",
    "                                width_shift_range = 0.2,\n",
    "                                height_shift_range = 0.2,\n",
    "                                fill_mode = 'nearest'\n",
    "                                )\n",
    "test_data = ImageDataGenerator(rescale = 1/.255)\n",
    "\n",
    "train_set = train_data.flow_from_directory('D:/amitavsir/bcd/datacompressed/train', #Image path\n",
    "                                           target_size = (im_size,im_size), #The shape which we want to input our images in our model\n",
    "                                           batch_size = 20,         #The batch size in mini batch gradient descent\n",
    "                                           class_mode = 'categorical')   \n",
    "\n",
    "\n",
    "test_set = test_data.flow_from_directory('D:/amitavsir/bcd/datacompressed/validation',\n",
    "                                         target_size = (im_size,im_size),\n",
    "                                         batch_size = 50,\n",
    "                                         class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds=10\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/20\n",
      "865/865 [==============================] - 74s 85ms/sample - loss: 4.6534 - acc: 0.4231\n",
      "Epoch 2/20\n",
      "865/865 [==============================] - 76s 87ms/sample - loss: 0.8507 - acc: 0.6520\n",
      "Epoch 3/20\n",
      "865/865 [==============================] - 79s 91ms/sample - loss: 0.6051 - acc: 0.7584\n",
      "Epoch 4/20\n",
      "865/865 [==============================] - 77s 89ms/sample - loss: 0.4473 - acc: 0.8347\n",
      "Epoch 5/20\n",
      "865/865 [==============================] - 99s 114ms/sample - loss: 0.3476 - acc: 0.8763\n",
      "Epoch 6/20\n",
      "865/865 [==============================] - 113s 130ms/sample - loss: 0.2814 - acc: 0.8867\n",
      "Epoch 7/20\n",
      "865/865 [==============================] - 114s 132ms/sample - loss: 0.2082 - acc: 0.9306\n",
      "Epoch 8/20\n",
      "865/865 [==============================] - 110s 127ms/sample - loss: 0.1638 - acc: 0.9422\n",
      "Epoch 9/20\n",
      "865/865 [==============================] - 119s 138ms/sample - loss: 0.1426 - acc: 0.9503\n",
      "Epoch 10/20\n",
      "865/865 [==============================] - 121s 139ms/sample - loss: 0.1109 - acc: 0.9653\n",
      "Epoch 11/20\n",
      "865/865 [==============================] - 122s 141ms/sample - loss: 0.1596 - acc: 0.9410\n",
      "Epoch 12/20\n",
      "865/865 [==============================] - 118s 137ms/sample - loss: 0.1062 - acc: 0.9549\n",
      "Epoch 13/20\n",
      "865/865 [==============================] - 118s 137ms/sample - loss: 0.0432 - acc: 0.9873\n",
      "Epoch 14/20\n",
      "865/865 [==============================] - 118s 137ms/sample - loss: 0.0658 - acc: 0.9780\n",
      "Epoch 15/20\n",
      "865/865 [==============================] - 117s 136ms/sample - loss: 0.0373 - acc: 0.9908\n",
      "Epoch 16/20\n",
      "865/865 [==============================] - 119s 138ms/sample - loss: 0.0262 - acc: 0.9884\n",
      "Epoch 17/20\n",
      "865/865 [==============================] - 116s 135ms/sample - loss: 0.0245 - acc: 0.9919\n",
      "Epoch 18/20\n",
      "865/865 [==============================] - 113s 131ms/sample - loss: 0.0184 - acc: 0.9931\n",
      "Epoch 19/20\n",
      "865/865 [==============================] - 118s 136ms/sample - loss: 0.0299 - acc: 0.9896\n",
      "Epoch 20/20\n",
      "865/865 [==============================] - 119s 138ms/sample - loss: 0.0690 - acc: 0.9780\n",
      "Score for fold 1: loss of 1.2046420439002345; acc of 77.31958627700806%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/20\n",
      "865/865 [==============================] - 122s 141ms/sample - loss: 3.7720 - acc: 0.4324\n",
      "Epoch 2/20\n",
      "865/865 [==============================] - 119s 137ms/sample - loss: 0.6614 - acc: 0.7468\n",
      "Epoch 3/20\n",
      "865/865 [==============================] - 118s 137ms/sample - loss: 0.5374 - acc: 0.7861\n",
      "Epoch 4/20\n",
      "865/865 [==============================] - 118s 137ms/sample - loss: 0.4341 - acc: 0.8312\n",
      "Epoch 5/20\n",
      "865/865 [==============================] - 118s 136ms/sample - loss: 0.3309 - acc: 0.8705\n",
      "Epoch 6/20\n",
      "865/865 [==============================] - 118s 137ms/sample - loss: 0.2924 - acc: 0.8902\n",
      "Epoch 7/20\n",
      "865/865 [==============================] - 107s 124ms/sample - loss: 0.2061 - acc: 0.9202\n",
      "Epoch 8/20\n",
      "865/865 [==============================] - 119s 138ms/sample - loss: 0.1675 - acc: 0.9364\n",
      "Epoch 9/20\n",
      "865/865 [==============================] - 119s 137ms/sample - loss: 0.1251 - acc: 0.9572\n",
      "Epoch 10/20\n",
      "865/865 [==============================] - 120s 139ms/sample - loss: 0.1154 - acc: 0.9607\n",
      "Epoch 11/20\n",
      "865/865 [==============================] - 123s 143ms/sample - loss: 0.0841 - acc: 0.9711\n",
      "Epoch 12/20\n",
      "865/865 [==============================] - 131s 152ms/sample - loss: 0.1685 - acc: 0.9422\n",
      "Epoch 13/20\n",
      "865/865 [==============================] - 129s 149ms/sample - loss: 0.1032 - acc: 0.9699\n",
      "Epoch 14/20\n",
      "865/865 [==============================] - 132s 152ms/sample - loss: 0.0533 - acc: 0.9827\n",
      "Epoch 15/20\n",
      "865/865 [==============================] - 131s 152ms/sample - loss: 0.0480 - acc: 0.9861\n",
      "Epoch 16/20\n",
      "865/865 [==============================] - 133s 154ms/sample - loss: 0.0607 - acc: 0.9769\n",
      "Epoch 17/20\n",
      "865/865 [==============================] - 80s 92ms/sample - loss: 0.0654 - acc: 0.9780\n",
      "Epoch 18/20\n",
      "865/865 [==============================] - 79s 91ms/sample - loss: 0.0584 - acc: 0.9803\n",
      "Epoch 19/20\n",
      "865/865 [==============================] - 82s 95ms/sample - loss: 0.0275 - acc: 0.9919\n",
      "Epoch 20/20\n",
      "865/865 [==============================] - 84s 97ms/sample - loss: 0.0476 - acc: 0.9803\n",
      "Score for fold 2: loss of 0.7988801358901348; acc of 81.44329786300659%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 79s 91ms/sample - loss: 5.7024 - acc: 0.4180\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 82s 95ms/sample - loss: 0.8465 - acc: 0.6259\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 80s 92ms/sample - loss: 0.5973 - acc: 0.7598\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 80s 93ms/sample - loss: 0.4236 - acc: 0.8383\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 83s 95ms/sample - loss: 0.3594 - acc: 0.8718\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 71s 82ms/sample - loss: 0.2624 - acc: 0.9088\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 80s 93ms/sample - loss: 0.2281 - acc: 0.9145\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 83s 96ms/sample - loss: 0.2000 - acc: 0.9249\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 80s 92ms/sample - loss: 0.1528 - acc: 0.9515\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 81s 94ms/sample - loss: 0.1134 - acc: 0.9630\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 82s 95ms/sample - loss: 0.0844 - acc: 0.9677\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 82s 94ms/sample - loss: 0.0547 - acc: 0.9850\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 78s 90ms/sample - loss: 0.0316 - acc: 0.9931\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 78s 91ms/sample - loss: 0.0298 - acc: 0.9873\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 77s 89ms/sample - loss: 0.0186 - acc: 0.9919\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 79s 91ms/sample - loss: 0.0200 - acc: 0.9954\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 70s 81ms/sample - loss: 0.0289 - acc: 0.9873\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 77s 88ms/sample - loss: 0.0473 - acc: 0.9827\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 78s 90ms/sample - loss: 0.0453 - acc: 0.9827\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 78s 90ms/sample - loss: 0.0214 - acc: 0.9931\n",
      "Score for fold 3: loss of 0.963069478670756; acc of 80.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 78s 90ms/sample - loss: 4.9944 - acc: 0.4088\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 76s 87ms/sample - loss: 0.7921 - acc: 0.7079\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 112s 129ms/sample - loss: 0.4944 - acc: 0.8141\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.3681 - acc: 0.8626\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.3310 - acc: 0.8764\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 110s 127ms/sample - loss: 0.2812 - acc: 0.8891\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.2003 - acc: 0.9307\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.1585 - acc: 0.9434\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.1268 - acc: 0.9503\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.1128 - acc: 0.9550\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 125s 145ms/sample - loss: 0.0958 - acc: 0.9677\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 126s 146ms/sample - loss: 0.1119 - acc: 0.9573\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 126s 145ms/sample - loss: 0.0886 - acc: 0.9688\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 123s 142ms/sample - loss: 0.0561 - acc: 0.9804\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 128s 148ms/sample - loss: 0.0470 - acc: 0.9815\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0193 - acc: 0.9965\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 127s 147ms/sample - loss: 0.0211 - acc: 0.9942\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 127s 147ms/sample - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 134s 155ms/sample - loss: 0.0160 - acc: 0.9965\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 134s 155ms/sample - loss: 0.0407 - acc: 0.9885\n",
      "Score for fold 4: loss of 0.8427749673525492; acc of 80.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 127s 147ms/sample - loss: 3.7523 - acc: 0.4215\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 121s 139ms/sample - loss: 0.7069 - acc: 0.7425\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 124s 144ms/sample - loss: 0.4922 - acc: 0.8152\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 123s 142ms/sample - loss: 0.4116 - acc: 0.8453\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 113s 131ms/sample - loss: 0.3327 - acc: 0.8614\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.2477 - acc: 0.9088\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 0.2212 - acc: 0.9169\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.1884 - acc: 0.9365\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.1578 - acc: 0.9353\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.1525 - acc: 0.9434\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 86s 100ms/sample - loss: 0.1069 - acc: 0.9630\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.0633 - acc: 0.9792\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.0475 - acc: 0.9815\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 79s 91ms/sample - loss: 0.0313 - acc: 0.9908\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 68s 78ms/sample - loss: 0.0456 - acc: 0.9861\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 58s 67ms/sample - loss: 0.0480 - acc: 0.9815\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 78s 90ms/sample - loss: 0.0394 - acc: 0.9815\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 77s 89ms/sample - loss: 0.0944 - acc: 0.9688\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 77s 89ms/sample - loss: 0.0657 - acc: 0.9827\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 76s 88ms/sample - loss: 0.0428 - acc: 0.9885\n",
      "Score for fold 5: loss of 0.8066079616546631; acc of 83.33333134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 81s 94ms/sample - loss: 4.4712 - acc: 0.4492\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 80s 92ms/sample - loss: 0.8174 - acc: 0.6686\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 78s 90ms/sample - loss: 0.5629 - acc: 0.7864\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 82s 94ms/sample - loss: 0.4483 - acc: 0.8326\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 64s 73ms/sample - loss: 0.3124 - acc: 0.8880\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 51s 59ms/sample - loss: 0.2798 - acc: 0.9053\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 52s 61ms/sample - loss: 0.2189 - acc: 0.9088\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 51s 59ms/sample - loss: 0.1830 - acc: 0.9273\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 49s 56ms/sample - loss: 0.1466 - acc: 0.9446\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 49s 57ms/sample - loss: 0.1569 - acc: 0.9434\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.0807 - acc: 0.9746\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 49s 57ms/sample - loss: 0.0854 - acc: 0.9584\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.0705 - acc: 0.9769\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 49s 56ms/sample - loss: 0.0499 - acc: 0.9838\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.0375 - acc: 0.9908\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.0633 - acc: 0.9804\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.0957 - acc: 0.9561\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 49s 56ms/sample - loss: 0.0695 - acc: 0.9746\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 49s 57ms/sample - loss: 0.0912 - acc: 0.9734\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 62s 71ms/sample - loss: 0.0332 - acc: 0.9885\n",
      "Score for fold 6: loss of 0.6523363987604777; acc of 84.375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 123s 142ms/sample - loss: 4.4298 - acc: 0.4111\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 124s 143ms/sample - loss: 0.8432 - acc: 0.6374\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 121s 140ms/sample - loss: 0.6184 - acc: 0.7448\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 123s 142ms/sample - loss: 0.4981 - acc: 0.8037\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 128s 148ms/sample - loss: 0.3850 - acc: 0.8476\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 127s 146ms/sample - loss: 0.2946 - acc: 0.8926\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 124s 143ms/sample - loss: 0.2553 - acc: 0.8995\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 129s 149ms/sample - loss: 0.2046 - acc: 0.9353\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 113s 131ms/sample - loss: 0.1581 - acc: 0.9376\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 99s 114ms/sample - loss: 0.1251 - acc: 0.9515\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 95s 110ms/sample - loss: 0.1457 - acc: 0.9434\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 62s 72ms/sample - loss: 0.0873 - acc: 0.9700\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 68s 79ms/sample - loss: 0.0706 - acc: 0.9758\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 49s 57ms/sample - loss: 0.0910 - acc: 0.9677\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 73s 85ms/sample - loss: 0.0693 - acc: 0.9792\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 101s 116ms/sample - loss: 0.1035 - acc: 0.9630\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0424 - acc: 0.9896\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.0296 - acc: 0.9896\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 0.0315 - acc: 0.9896\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0385 - acc: 0.9850\n",
      "Score for fold 7: loss of 0.7701096932093302; acc of 86.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 123s 142ms/sample - loss: 4.5292 - acc: 0.4180\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.8171 - acc: 0.6836\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 114s 132ms/sample - loss: 0.5302 - acc: 0.7852\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 108s 125ms/sample - loss: 0.3899 - acc: 0.8522\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.3352 - acc: 0.8730\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.2832 - acc: 0.8972\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.1971 - acc: 0.9215\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.1827 - acc: 0.9388\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 117s 136ms/sample - loss: 0.1433 - acc: 0.9423\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.1145 - acc: 0.9630\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.0938 - acc: 0.9723\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.1016 - acc: 0.9607\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 113s 130ms/sample - loss: 0.0791 - acc: 0.9723\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 115s 132ms/sample - loss: 0.0574 - acc: 0.9827\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0466 - acc: 0.9827\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.0912 - acc: 0.9723\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.1263 - acc: 0.9550\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0438 - acc: 0.9827\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0326 - acc: 0.9885\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.0225 - acc: 0.9919\n",
      "Score for fold 8: loss of 0.7997346719106039; acc of 82.29166865348816%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 121s 139ms/sample - loss: 3.9894 - acc: 0.4319\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 109s 126ms/sample - loss: 0.7154 - acc: 0.7263\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.4885 - acc: 0.8222\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 101s 117ms/sample - loss: 0.3714 - acc: 0.8603\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.4212 - acc: 0.8268\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 75s 86ms/sample - loss: 0.2946 - acc: 0.8915\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 75s 86ms/sample - loss: 0.1943 - acc: 0.9238\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.1573 - acc: 0.9353\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 75s 87ms/sample - loss: 0.1291 - acc: 0.9503\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 75s 87ms/sample - loss: 0.1226 - acc: 0.9527\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 75s 87ms/sample - loss: 0.0746 - acc: 0.9677\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 66s 76ms/sample - loss: 0.0603 - acc: 0.9792\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.0834 - acc: 0.9758\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.0776 - acc: 0.9734\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.0381 - acc: 0.9896\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.0284 - acc: 0.9896\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.0292 - acc: 0.9919\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 75s 86ms/sample - loss: 0.0560 - acc: 0.9838\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.0421 - acc: 0.9815\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.2191 - acc: 0.9261\n",
      "Score for fold 9: loss of 0.6234900951385498; acc of 80.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/20\n",
      "866/866 [==============================] - 57s 66ms/sample - loss: 5.3443 - acc: 0.3857\n",
      "Epoch 2/20\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.7643 - acc: 0.7009\n",
      "Epoch 3/20\n",
      "866/866 [==============================] - 51s 58ms/sample - loss: 0.5394 - acc: 0.7887\n",
      "Epoch 4/20\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.4271 - acc: 0.8464\n",
      "Epoch 5/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.3194 - acc: 0.8741\n",
      "Epoch 6/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.3046 - acc: 0.8857\n",
      "Epoch 7/20\n",
      "866/866 [==============================] - 73s 85ms/sample - loss: 0.2659 - acc: 0.9076\n",
      "Epoch 8/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.2686 - acc: 0.8926\n",
      "Epoch 9/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.2035 - acc: 0.9365\n",
      "Epoch 10/20\n",
      "866/866 [==============================] - 73s 85ms/sample - loss: 0.1209 - acc: 0.9561\n",
      "Epoch 11/20\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.0806 - acc: 0.9746\n",
      "Epoch 12/20\n",
      "866/866 [==============================] - 73s 84ms/sample - loss: 0.1064 - acc: 0.9573\n",
      "Epoch 13/20\n",
      "866/866 [==============================] - 64s 74ms/sample - loss: 0.0610 - acc: 0.9838\n",
      "Epoch 14/20\n",
      "866/866 [==============================] - 73s 84ms/sample - loss: 0.0503 - acc: 0.9792\n",
      "Epoch 15/20\n",
      "866/866 [==============================] - 73s 85ms/sample - loss: 0.0960 - acc: 0.9700\n",
      "Epoch 16/20\n",
      "866/866 [==============================] - 70s 81ms/sample - loss: 0.1435 - acc: 0.9492\n",
      "Epoch 17/20\n",
      "866/866 [==============================] - 71s 82ms/sample - loss: 0.0738 - acc: 0.9769\n",
      "Epoch 18/20\n",
      "866/866 [==============================] - 73s 84ms/sample - loss: 0.0436 - acc: 0.9873\n",
      "Epoch 19/20\n",
      "866/866 [==============================] - 71s 82ms/sample - loss: 0.0301 - acc: 0.9908\n",
      "Epoch 20/20\n",
      "866/866 [==============================] - 71s 82ms/sample - loss: 0.0518 - acc: 0.9861\n",
      "Score for fold 10: loss of 0.791322261095047; acc of 85.41666865348816%\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(inputs, targets):\n",
    "    #Building own model on top of trained network\n",
    "    x = Conv2D(500, (1,1), activation = 'relu')(last)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1028, activation = 'relu')(x)\n",
    "    x = Dropout(rate = 0.2)(x)\n",
    "    x = Dense(512, activation = 'relu')(x)\n",
    "    x = Dropout(rate = 0.25)(x)\n",
    "    x = Dense(4, activation = 'softmax')(x)\n",
    "\n",
    "    #Compiling model\n",
    "    model = Model(inputs = pre_trained_model.input, outputs = x, name = 'Predict')\n",
    "    opt1 = Adam(learning_rate = 0.001)\n",
    "    opt2 = RMSprop(learning_rate = 0.001)\n",
    "    model.compile(optimizer = opt1 , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=no_epochs,\n",
    "              verbose=verbosity,\n",
    "              #validation_data = test_set,\n",
    "              #validation_steps = 5          \n",
    "              )\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 21.081590298524837 - Accuracy: 38.14432919025421%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 21.368139070333893 - Accuracy: 23.711340129375458%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 21.17801030476888 - Accuracy: 25.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 21.301259358723957 - Accuracy: 28.125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 21.314400990804035 - Accuracy: 15.625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 21.138458887736004 - Accuracy: 25.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 21.196800867716473 - Accuracy: 19.79166716337204%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 21.192534764607746 - Accuracy: 18.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 21.2336483001709 - Accuracy: 31.25%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 21.182409286499023 - Accuracy: 17.70833283662796%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 21.17275359950115 - Accuracy: 27.835050225257874%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 21.29630063243748 - Accuracy: 17.525772750377655%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 21.187467575073242 - Accuracy: 17.70833283662796%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 21.37045733133952 - Accuracy: 29.16666567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 21.293339411417644 - Accuracy: 17.70833283662796%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 21.203259910504844 - Accuracy: 17.525772750377655%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 21.16664272976905 - Accuracy: 31.95876181125641%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 21.11383310953776 - Accuracy: 19.79166716337204%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 21.19607416788737 - Accuracy: 21.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 21.32158660888672 - Accuracy: 20.83333283662796%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 21.047325134277344 - Accuracy: 36.45833432674408%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 21.22448221842448 - Accuracy: 31.25%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 21.11125310262044 - Accuracy: 43.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 21.23531977335612 - Accuracy: 18.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 21.31537119547526 - Accuracy: 30.20833432674408%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 1.2046420439002345 - Accuracy: 77.31958627700806%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 0.7988801358901348 - Accuracy: 81.44329786300659%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 0.963069478670756 - Accuracy: 80.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 0.8427749673525492 - Accuracy: 80.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 0.8066079616546631 - Accuracy: 83.33333134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.6523363987604777 - Accuracy: 84.375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 0.7701096932093302 - Accuracy: 86.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 0.7997346719106039 - Accuracy: 82.29166865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 0.6234900951385498 - Accuracy: 80.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 0.791322261095047 - Accuracy: 85.41666865348816%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 41.33468300104141 (+- 26.5818748398161)\n",
      "> Loss: 15.391305323942188\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
