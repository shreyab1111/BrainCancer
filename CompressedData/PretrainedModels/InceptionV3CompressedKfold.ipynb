{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import PIL\n",
    "#Pretrained model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "#FEEDING DATA INTO THE MODEL\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #allows us to augment and process data fed into a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "path_inception = \"D:/amitavsir/bcd/data/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "local_weights_file = path_inception\n",
    "pre_trained_model = InceptionV3(input_shape = (121, 121, 3), \n",
    "                                include_top = False, \n",
    "                                weights = None)\n",
    "pre_trained_model.load_weights(local_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last layer output shape :  (?, 2, 2, 1280)\n"
     ]
    }
   ],
   "source": [
    "#Locking the weights and parameters of pretrained model\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Getting desired layer output\n",
    "last_layer = pre_trained_model.get_layer('mixed8')\n",
    "last = last_layer.output\n",
    "print('Last layer output shape : ', last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "# Model configuration\n",
    "batch_size = 50\n",
    "img_size, img_num_channels = 121, 3\n",
    "loss_function = categorical_crossentropy\n",
    "no_classes = 4\n",
    "no_epochs = 15\n",
    "optimizer = Adam()\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os,cv2,random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='D:/amitavsir/bcd/datacompressed/train'\n",
    "list_folder=os.listdir(path = data_path)\n",
    "data=[]\n",
    "im_size=121    \n",
    "for i in list_folder:\n",
    "    new_path=os.path.join(data_path,i) \n",
    "    pic_list=os.listdir(new_path)                                               \n",
    "    for img in pic_list:\n",
    "        pic=os.path.join(new_path,img)   \n",
    "        arr=cv2.imread(pic)    \n",
    "        data.append([arr,list_folder.index(i)])    \n",
    "        \n",
    "random.shuffle(data)  \n",
    "x_train,y_train=[],[]\n",
    "for i,j in data:\n",
    "    x_train.append(i)\n",
    "    y_train.append(j)\n",
    "x_train=np.array(x_train).reshape(-1,im_size,im_size,3)\n",
    "y_train=np.array(y_train).reshape(-1,1)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "X_train = x_train/255\n",
    "y_train = y_train.toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train,y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse numbers as floats\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = [] #stores accuracy\n",
    "loss_per_fold = [] #stores losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 962 images belonging to 4 classes.\n",
      "Found 500 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#FEEDING DATA INTO THE MODEL\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #A tool that allows us to augment and process data to be fed into a CNN\n",
    "\n",
    "#class for training set\n",
    "train_data = ImageDataGenerator(rescale = 1./255, #pixel values rescaled so that it stays between 0 and 1.\n",
    "                                shear_range = 0.2, #this and the next 4 steps are used to augment our data by shearing it, flipping and zooming it to produce more examples per image.\n",
    "                                zoom_range = 0.2,  #Data augmentation allows us to prevent overfitting of data into the training set.\n",
    "                                horizontal_flip = True,\n",
    "                                vertical_flip = True,\n",
    "                                rotation_range = 40,\n",
    "                                width_shift_range = 0.2,\n",
    "                                height_shift_range = 0.2,\n",
    "                                fill_mode = 'nearest'\n",
    "                                )\n",
    "test_data = ImageDataGenerator(rescale = 1/.255)\n",
    "\n",
    "train_set = train_data.flow_from_directory('D:/amitavsir/bcd/datacompressed/train', #Image path\n",
    "                                           target_size = (121,121), #The shape which we want to input our images in our model\n",
    "                                           batch_size = 20,         #The batch size in mini batch gradient descent\n",
    "                                           class_mode = 'categorical')   \n",
    "\n",
    "\n",
    "test_set = test_data.flow_from_directory('D:/amitavsir/bcd/datacompressed/validation',\n",
    "                                         target_size = (121,121),\n",
    "                                         batch_size = 50,\n",
    "                                         class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "num_folds=10\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/15\n",
      "865/865 [==============================] - 81s 94ms/sample - loss: 1.6728 - acc: 0.4682\n",
      "Epoch 2/15\n",
      "865/865 [==============================] - 73s 85ms/sample - loss: 0.7284 - acc: 0.7272\n",
      "Epoch 3/15\n",
      "865/865 [==============================] - 73s 84ms/sample - loss: 0.5290 - acc: 0.8116\n",
      "Epoch 4/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.4487 - acc: 0.8173\n",
      "Epoch 5/15\n",
      "865/865 [==============================] - 74s 86ms/sample - loss: 0.2766 - acc: 0.9006\n",
      "Epoch 6/15\n",
      "865/865 [==============================] - 70s 81ms/sample - loss: 0.1998 - acc: 0.9306\n",
      "Epoch 7/15\n",
      "865/865 [==============================] - 72s 84ms/sample - loss: 0.2076 - acc: 0.9214\n",
      "Epoch 8/15\n",
      "865/865 [==============================] - 56s 64ms/sample - loss: 0.1592 - acc: 0.9434\n",
      "Epoch 9/15\n",
      "865/865 [==============================] - 53s 61ms/sample - loss: 0.2092 - acc: 0.9214\n",
      "Epoch 10/15\n",
      "865/865 [==============================] - 50s 58ms/sample - loss: 0.1565 - acc: 0.9457\n",
      "Epoch 11/15\n",
      "865/865 [==============================] - 64s 74ms/sample - loss: 0.1776 - acc: 0.9457\n",
      "Epoch 12/15\n",
      "865/865 [==============================] - 73s 84ms/sample - loss: 0.0899 - acc: 0.9723\n",
      "Epoch 13/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.0645 - acc: 0.9780\n",
      "Epoch 14/15\n",
      "865/865 [==============================] - 73s 84ms/sample - loss: 0.1834 - acc: 0.9387\n",
      "Epoch 15/15\n",
      "865/865 [==============================] - 74s 85ms/sample - loss: 0.1013 - acc: 0.9688\n",
      "Score for fold 1: loss of 2.4479335902892436; acc of 54.63917255401611%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/15\n",
      "865/865 [==============================] - 81s 94ms/sample - loss: 1.6187 - acc: 0.4092\n",
      "Epoch 2/15\n",
      "865/865 [==============================] - 72s 84ms/sample - loss: 0.7324 - acc: 0.7422\n",
      "Epoch 3/15\n",
      "865/865 [==============================] - 75s 87ms/sample - loss: 0.5235 - acc: 0.8162\n",
      "Epoch 4/15\n",
      "865/865 [==============================] - 72s 84ms/sample - loss: 0.3654 - acc: 0.8751\n",
      "Epoch 5/15\n",
      "865/865 [==============================] - 73s 84ms/sample - loss: 0.2641 - acc: 0.9006\n",
      "Epoch 6/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.2220 - acc: 0.9156\n",
      "Epoch 7/15\n",
      "865/865 [==============================] - 74s 86ms/sample - loss: 0.2056 - acc: 0.9318\n",
      "Epoch 8/15\n",
      "865/865 [==============================] - 75s 87ms/sample - loss: 0.1626 - acc: 0.9410\n",
      "Epoch 9/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.1469 - acc: 0.9561\n",
      "Epoch 10/15\n",
      "865/865 [==============================] - 73s 85ms/sample - loss: 0.2134 - acc: 0.9341\n",
      "Epoch 11/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.0878 - acc: 0.9757\n",
      "Epoch 12/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.0782 - acc: 0.9711\n",
      "Epoch 13/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.0908 - acc: 0.9653\n",
      "Epoch 14/15\n",
      "865/865 [==============================] - 72s 83ms/sample - loss: 0.0767 - acc: 0.9734\n",
      "Epoch 15/15\n",
      "865/865 [==============================] - 73s 85ms/sample - loss: 0.0643 - acc: 0.9803\n",
      "Score for fold 2: loss of 8.0428888379913; acc of 45.36082446575165%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 84s 97ms/sample - loss: 1.7872 - acc: 0.4734\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 73s 84ms/sample - loss: 0.6663 - acc: 0.7552\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.5036 - acc: 0.8072\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 72s 83ms/sample - loss: 0.3444 - acc: 0.8718\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.2559 - acc: 0.9122\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 72s 83ms/sample - loss: 0.2135 - acc: 0.9273\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 73s 85ms/sample - loss: 0.1769 - acc: 0.9319\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.1630 - acc: 0.9411\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 73s 84ms/sample - loss: 0.1805 - acc: 0.9284\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 74s 86ms/sample - loss: 0.1513 - acc: 0.9400\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 72s 83ms/sample - loss: 0.1330 - acc: 0.9607\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 57s 66ms/sample - loss: 0.1062 - acc: 0.9584\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 50s 58ms/sample - loss: 0.1221 - acc: 0.9630\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 63s 73ms/sample - loss: 0.0778 - acc: 0.9734\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 74s 85ms/sample - loss: 0.1674 - acc: 0.9434\n",
      "Score for fold 3: loss of 3.975273370742798; acc of 38.54166567325592%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 81s 94ms/sample - loss: 1.8300 - acc: 0.3476\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 71s 82ms/sample - loss: 0.7537 - acc: 0.7206\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 75s 87ms/sample - loss: 0.4883 - acc: 0.8222\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 62s 71ms/sample - loss: 0.4198 - acc: 0.8418\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.3302 - acc: 0.8788\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.2587 - acc: 0.9053\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.2289 - acc: 0.9203\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.2222 - acc: 0.9249\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.1929 - acc: 0.9342\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.2383 - acc: 0.9203\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.0985 - acc: 0.9642\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.0990 - acc: 0.9642\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.1169 - acc: 0.9688\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 51s 58ms/sample - loss: 0.0686 - acc: 0.9734\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 49s 57ms/sample - loss: 0.0640 - acc: 0.9792\n",
      "Score for fold 4: loss of 4.052680015563965; acc of 47.91666567325592%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 52s 60ms/sample - loss: 1.5773 - acc: 0.4723\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.6200 - acc: 0.7771\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.4488 - acc: 0.8418\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.3282 - acc: 0.8822\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.2703 - acc: 0.8926\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.2009 - acc: 0.9180\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.1339 - acc: 0.9561\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.2738 - acc: 0.9111\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.1943 - acc: 0.9330\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.1190 - acc: 0.9596\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.0918 - acc: 0.9746\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.0593 - acc: 0.9804\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.0417 - acc: 0.9827\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 36s 42ms/sample - loss: 0.1247 - acc: 0.9596\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 31s 36ms/sample - loss: 0.0717 - acc: 0.9734\n",
      "Score for fold 5: loss of 2.7550979455312095; acc of 55.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 52s 60ms/sample - loss: 1.4957 - acc: 0.5104\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 48s 56ms/sample - loss: 0.6357 - acc: 0.7679\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.4422 - acc: 0.8222\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.3015 - acc: 0.8891\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 48s 55ms/sample - loss: 0.2425 - acc: 0.9099\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.2171 - acc: 0.9261\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.1942 - acc: 0.9376\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.1691 - acc: 0.9434\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.1985 - acc: 0.9353\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.2341 - acc: 0.9249\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.1248 - acc: 0.9550\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 49s 56ms/sample - loss: 0.0941 - acc: 0.9734\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.0862 - acc: 0.9711\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.0645 - acc: 0.9815\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 46s 54ms/sample - loss: 0.0485 - acc: 0.9850\n",
      "Score for fold 6: loss of 5.712203820546468; acc of 35.41666567325592%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 51s 59ms/sample - loss: 1.4240 - acc: 0.5000\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.6294 - acc: 0.7737\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 47s 54ms/sample - loss: 0.4103 - acc: 0.8349\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 46s 53ms/sample - loss: 0.3850 - acc: 0.8533\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 45s 52ms/sample - loss: 0.2985 - acc: 0.8926\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 46s 53ms/sample - loss: 0.3055 - acc: 0.8915\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.1892 - acc: 0.9238\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 47s 55ms/sample - loss: 0.1782 - acc: 0.9353\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 46s 53ms/sample - loss: 0.1820 - acc: 0.9353\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 45s 52ms/sample - loss: 0.1664 - acc: 0.9388\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 42s 49ms/sample - loss: 0.0845 - acc: 0.9711\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 25s 29ms/sample - loss: 0.0751 - acc: 0.9677\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 22s 25ms/sample - loss: 0.0705 - acc: 0.9781\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.0660 - acc: 0.9723\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.0558 - acc: 0.9838\n",
      "Score for fold 7: loss of 6.534671942392985; acc of 39.58333432674408%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 1.7435 - acc: 0.4388\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.6459 - acc: 0.7552\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.4546 - acc: 0.8256\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.3440 - acc: 0.8788\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.2741 - acc: 0.8972\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.2134 - acc: 0.9169\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 21s 24ms/sample - loss: 0.2375 - acc: 0.9145\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.1538 - acc: 0.9515\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1774 - acc: 0.9330\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1558 - acc: 0.9423\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.2137 - acc: 0.9307\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0875 - acc: 0.9688\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0459 - acc: 0.9804\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0636 - acc: 0.9781\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0615 - acc: 0.9769\n",
      "Score for fold 8: loss of 4.068822026252747; acc of 45.83333432674408%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 2.0141 - acc: 0.4376\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.7786 - acc: 0.7148\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.5251 - acc: 0.8025\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.4171 - acc: 0.8510\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.3147 - acc: 0.8822\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.2114 - acc: 0.9273\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.2894 - acc: 0.9007\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.2219 - acc: 0.9180\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1198 - acc: 0.9607\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0897 - acc: 0.9723\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0616 - acc: 0.9815\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 0.0574 - acc: 0.9815\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0725 - acc: 0.9734\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1033 - acc: 0.9758\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1066 - acc: 0.9573\n",
      "Score for fold 9: loss of 4.450002193450928; acc of 32.29166567325592%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 20s 23ms/sample - loss: 1.6980 - acc: 0.4492\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.7783 - acc: 0.6871\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.5397 - acc: 0.7910\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.3898 - acc: 0.8453\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.3082 - acc: 0.8834\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.2657 - acc: 0.8984\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.2510 - acc: 0.9111\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1431 - acc: 0.9469\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.2491 - acc: 0.9180\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1625 - acc: 0.9480\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1289 - acc: 0.9607\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.1163 - acc: 0.9654\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0647 - acc: 0.9723\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0594 - acc: 0.9873\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 19s 22ms/sample - loss: 0.0451 - acc: 0.9861\n",
      "Score for fold 10: loss of 6.195111592610677; acc of 47.91666567325592%\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(inputs, targets):\n",
    "    #Building own model on top of trained network\n",
    "    x = Conv2D(600, (1,1), activation = 'relu')(last)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1028, activation = 'relu')(x)\n",
    "    x = Dropout(rate = 0.3)(x)\n",
    "    x = Dense(512, activation = 'relu')(x)\n",
    "    x = Dropout(rate = 0.25)(x)\n",
    "    x = Dense(4, activation = 'softmax')(x)\n",
    "\n",
    "    #Compiling model\n",
    "    model = Model(inputs = pre_trained_model.input, outputs = x, name = 'Predict')\n",
    "    opt1 = Adam(learning_rate = 0.002)\n",
    "    opt2 = RMSprop(learning_rate = 0.001)\n",
    "    model.compile(optimizer = opt1 , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=no_epochs,\n",
    "              verbose=verbosity,        \n",
    "              )\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 4.714763336574908 - Accuracy: 39.175257086753845%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 3.3151804638892104 - Accuracy: 54.63917255401611%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.710722697149847 - Accuracy: 50.51546096801758%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.9355133374532065 - Accuracy: 44.79166567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.69365914662679 - Accuracy: 43.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 5.119016965230306 - Accuracy: 47.91666567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.250235557556152 - Accuracy: 34.375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 5.912539482116699 - Accuracy: 35.41666567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.567225535710653 - Accuracy: 48.95833432674408%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 9.627425829569498 - Accuracy: 28.125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 5.650309721628825 - Accuracy: 38.54166567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 11.209476765898085 - Accuracy: 34.020617604255676%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 2.4479335902892436 - Accuracy: 54.63917255401611%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 8.0428888379913 - Accuracy: 45.36082446575165%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 3.975273370742798 - Accuracy: 38.54166567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 4.052680015563965 - Accuracy: 47.91666567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 2.7550979455312095 - Accuracy: 55.20833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 5.712203820546468 - Accuracy: 35.41666567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 6.534671942392985 - Accuracy: 39.58333432674408%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 4.068822026252747 - Accuracy: 45.83333432674408%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 4.450002193450928 - Accuracy: 32.29166567325592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 6.195111592610677 - Accuracy: 47.91666567325592%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 42.86062866449356 (+- 7.608186065367841)\n",
      "> Loss: 5.270034280671658\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
