{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING THE BRAIN TUMOR MODEL WITH INCEPTION V3 PRETRAINED MODEL\n",
    "\n",
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import PIL\n",
    "#Pretrained model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "#FEEDING DATA INTO THE MODEL\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #allows us to augment and process data fed into a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_inception = \"D:/bcd/data/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "local_weights_file = path_inception\n",
    "pre_trained_model = InceptionV3(input_shape = (240, 240, 3), \n",
    "                                include_top = False, \n",
    "                                weights = None)\n",
    "pre_trained_model.load_weights(local_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last layer output shape :  (?, 6, 6, 1280)\n"
     ]
    }
   ],
   "source": [
    "#Locking the weights and parameters of pretrained model\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Getting desired layer output\n",
    "last_layer = pre_trained_model.get_layer('mixed8')\n",
    "last = last_layer.output\n",
    "print('Last layer output shape : ', last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "# Model configuration\n",
    "batch_size = 50\n",
    "img_size, img_num_channels = 240, 3\n",
    "loss_function = categorical_crossentropy\n",
    "no_classes = 4\n",
    "no_epochs = 15\n",
    "optimizer = Adam()\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os,cv2,random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='D:/bcd/data/trainingImages'\n",
    "list_folder=os.listdir(path = data_path)\n",
    "data=[]\n",
    "im_size=240    \n",
    "for i in list_folder:\n",
    "    new_path=os.path.join(data_path,i) \n",
    "    pic_list=os.listdir(new_path)                                               \n",
    "    for img in pic_list:\n",
    "        pic=os.path.join(new_path,img)   \n",
    "        arr=cv2.imread(pic)    \n",
    "        data.append([arr,list_folder.index(i)])    \n",
    "        \n",
    "random.shuffle(data)  \n",
    "x_train,y_train=[],[]\n",
    "for i,j in data:\n",
    "    x_train.append(i)\n",
    "    y_train.append(j)\n",
    "x_train=np.array(x_train).reshape(-1,im_size,im_size,3)\n",
    "y_train=np.array(y_train).reshape(-1,1)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "X_train = x_train/255\n",
    "y_train = y_train.toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train,y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse numbers as floats\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = [] #stores accuracy\n",
    "loss_per_fold = [] #stores losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 962 images belonging to 4 classes.\n",
      "Found 500 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#FEEDING DATA INTO THE MODEL\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #A tool that allows us to augment and process data to be fed into a CNN\n",
    "\n",
    "#class for training set\n",
    "train_data = ImageDataGenerator(rescale = 1./255, #pixel values rescaled so that it stays between 0 and 1.\n",
    "                                shear_range = 0.2, #this and the next 4 steps are used to augment our data by shearing it, flipping and zooming it to produce more examples per image.\n",
    "                                zoom_range = 0.2,  #Data augmentation allows us to prevent overfitting of data into the training set.\n",
    "                                horizontal_flip = True,\n",
    "                                vertical_flip = True,\n",
    "                                rotation_range = 40,\n",
    "                                width_shift_range = 0.2,\n",
    "                                height_shift_range = 0.2,\n",
    "                                fill_mode = 'nearest'\n",
    "                                )\n",
    "test_data = ImageDataGenerator(rescale = 1/.255)\n",
    "\n",
    "train_set = train_data.flow_from_directory('D:/bcd/data/trainingImages', #Image path\n",
    "                                           target_size = (240,240), #The shape which we want to input our images in our model\n",
    "                                           batch_size = 20,         #The batch size in mini batch gradient descent\n",
    "                                           class_mode = 'categorical')   \n",
    "\n",
    "\n",
    "test_set = test_data.flow_from_directory('D:/bcd/data/validationImages',\n",
    "                                         target_size = (240,240),\n",
    "                                         batch_size = 50,\n",
    "                                         class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds=10\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/15\n",
      "865/865 [==============================] - 262s 303ms/sample - loss: 3.5437 - acc: 0.2983\n",
      "Epoch 2/15\n",
      "865/865 [==============================] - 306s 354ms/sample - loss: 0.7271 - acc: 0.7156\n",
      "Epoch 3/15\n",
      "865/865 [==============================] - 305s 352ms/sample - loss: 0.3221 - acc: 0.8936\n",
      "Epoch 4/15\n",
      "865/865 [==============================] - 299s 346ms/sample - loss: 0.1631 - acc: 0.9376\n",
      "Epoch 5/15\n",
      "865/865 [==============================] - 313s 361ms/sample - loss: 0.0878 - acc: 0.9723\n",
      "Epoch 6/15\n",
      "865/865 [==============================] - 263s 304ms/sample - loss: 0.0392 - acc: 0.9861\n",
      "Epoch 7/15\n",
      "865/865 [==============================] - 292s 338ms/sample - loss: 0.0154 - acc: 0.9954\n",
      "Epoch 8/15\n",
      "865/865 [==============================] - 295s 341ms/sample - loss: 0.0218 - acc: 0.9919\n",
      "Epoch 9/15\n",
      "865/865 [==============================] - 290s 335ms/sample - loss: 0.0177 - acc: 0.9954\n",
      "Epoch 10/15\n",
      "865/865 [==============================] - 219s 253ms/sample - loss: 0.0445 - acc: 0.9792\n",
      "Epoch 11/15\n",
      "865/865 [==============================] - 244s 282ms/sample - loss: 0.0404 - acc: 0.9873\n",
      "Epoch 12/15\n",
      "865/865 [==============================] - 243s 281ms/sample - loss: 0.0123 - acc: 0.9965\n",
      "Epoch 13/15\n",
      "865/865 [==============================] - 271s 313ms/sample - loss: 0.0447 - acc: 0.9792\n",
      "Epoch 14/15\n",
      "865/865 [==============================] - 253s 293ms/sample - loss: 0.0434 - acc: 0.9850\n",
      "Epoch 15/15\n",
      "865/865 [==============================] - 307s 355ms/sample - loss: 0.0198 - acc: 0.9908\n",
      "Score for fold 1: loss of 0.5871885579425035; acc of 81.44329786300659%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/15\n",
      "865/865 [==============================] - 246s 284ms/sample - loss: 2.7494 - acc: 0.4763\n",
      "Epoch 2/15\n",
      "865/865 [==============================] - 243s 281ms/sample - loss: 0.4595 - acc: 0.8324\n",
      "Epoch 3/15\n",
      "865/865 [==============================] - 241s 279ms/sample - loss: 0.1945 - acc: 0.9260\n",
      "Epoch 4/15\n",
      "865/865 [==============================] - 239s 277ms/sample - loss: 0.0673 - acc: 0.9757\n",
      "Epoch 5/15\n",
      "865/865 [==============================] - 238s 275ms/sample - loss: 0.0216 - acc: 0.9942\n",
      "Epoch 6/15\n",
      "865/865 [==============================] - 239s 276ms/sample - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 7/15\n",
      "865/865 [==============================] - 230s 265ms/sample - loss: 0.0436 - acc: 0.9884\n",
      "Epoch 8/15\n",
      "865/865 [==============================] - 237s 274ms/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 9/15\n",
      "865/865 [==============================] - 242s 280ms/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 10/15\n",
      "865/865 [==============================] - 243s 281ms/sample - loss: 0.0020 - acc: 0.9988\n",
      "Epoch 11/15\n",
      "865/865 [==============================] - 242s 280ms/sample - loss: 0.0124 - acc: 0.9965\n",
      "Epoch 12/15\n",
      "865/865 [==============================] - 242s 279ms/sample - loss: 0.0059 - acc: 0.9977\n",
      "Epoch 13/15\n",
      "865/865 [==============================] - 230s 266ms/sample - loss: 0.0030 - acc: 0.9988\n",
      "Epoch 14/15\n",
      "865/865 [==============================] - 239s 277ms/sample - loss: 0.0022 - acc: 0.9988\n",
      "Epoch 15/15\n",
      "865/865 [==============================] - 239s 276ms/sample - loss: 0.0025 - acc: 0.9988\n",
      "Score for fold 2: loss of 0.38549215523202407; acc of 90.7216489315033%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 244s 282ms/sample - loss: 2.4609 - acc: 0.4053\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 244s 282ms/sample - loss: 0.5148 - acc: 0.8141\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 237s 273ms/sample - loss: 0.1583 - acc: 0.9480\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 242s 279ms/sample - loss: 0.0805 - acc: 0.9688\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 239s 276ms/sample - loss: 0.0559 - acc: 0.9792\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 238s 274ms/sample - loss: 0.0258 - acc: 0.9919\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 240s 277ms/sample - loss: 0.0256 - acc: 0.9908\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 234s 270ms/sample - loss: 0.0171 - acc: 0.9965\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 238s 275ms/sample - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 452s 522ms/sample - loss: 0.0171 - acc: 0.9931\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 228s 263ms/sample - loss: 0.0079 - acc: 0.9977\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 257s 296ms/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 255s 295ms/sample - loss: 0.0015 - acc: 0.9988\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 206s 238ms/sample - loss: 8.2098e-04 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 117s 136ms/sample - loss: 0.0014 - acc: 1.0000\n",
      "Score for fold 3: loss of 3.5370729764302573; acc of 61.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 107s 123ms/sample - loss: 2.3211 - acc: 0.5427\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 105s 121ms/sample - loss: 0.3373 - acc: 0.8857\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 115s 133ms/sample - loss: 0.1281 - acc: 0.9550\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.0494 - acc: 0.9861\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 122s 141ms/sample - loss: 0.0159 - acc: 0.9954\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 0.0097 - acc: 0.9988\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0850 - acc: 0.9711\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.0590 - acc: 0.9804\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 112s 129ms/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 115s 133ms/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0078 - acc: 0.9965\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.0014 - acc: 1.0000\n",
      "Score for fold 4: loss of 0.2696586698293686; acc of 91.66666865348816%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 113s 130ms/sample - loss: 2.6373 - acc: 0.4896\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.3461 - acc: 0.8718\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.1715 - acc: 0.9353\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 121s 139ms/sample - loss: 0.0670 - acc: 0.9769\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 127s 147ms/sample - loss: 0.0414 - acc: 0.9850\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 122s 141ms/sample - loss: 0.0238 - acc: 0.9896\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 0.0179 - acc: 0.9919\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 126s 145ms/sample - loss: 0.0252 - acc: 0.9919\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 128s 148ms/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 125s 144ms/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 122s 141ms/sample - loss: 0.0031 - acc: 0.9988\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.0125 - acc: 0.9942\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.0051 - acc: 0.9988\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 124s 144ms/sample - loss: 6.5971e-04 - acc: 1.0000\n",
      "Score for fold 5: loss of 1.6566379070281982; acc of 61.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 115s 133ms/sample - loss: 3.3296 - acc: 0.2898\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.6622 - acc: 0.7529\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 125s 144ms/sample - loss: 0.1983 - acc: 0.9376\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.0812 - acc: 0.9734\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0459 - acc: 0.9861\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.0204 - acc: 0.9931\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.0131 - acc: 0.9942\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.0179 - acc: 0.9931\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 121s 139ms/sample - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 0.0026 - acc: 0.9988\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.0031 - acc: 0.9988\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 121s 139ms/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.0037 - acc: 0.9977\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.0116 - acc: 0.9965\n",
      "Score for fold 6: loss of 1.581248164176941; acc of 67.70833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 116s 134ms/sample - loss: 3.1012 - acc: 0.3383\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.5735 - acc: 0.8095\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 119s 137ms/sample - loss: 0.1945 - acc: 0.9365\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.1063 - acc: 0.9700\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0329 - acc: 0.9908\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0129 - acc: 0.9965\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0053 - acc: 0.9988\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 115s 133ms/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 113s 131ms/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 2.3015e-04 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 3.4507e-04 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 6.4328e-04 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.0051 - acc: 0.9977\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.0119 - acc: 0.9965\n",
      "Score for fold 7: loss of 2.7730469703674316; acc of 50.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 120s 138ms/sample - loss: 2.7723 - acc: 0.3845\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.6554 - acc: 0.7864\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 117s 135ms/sample - loss: 0.2727 - acc: 0.9169\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0905 - acc: 0.9711\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0429 - acc: 0.9850\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0778 - acc: 0.9723\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 117s 136ms/sample - loss: 0.0595 - acc: 0.9781\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0193 - acc: 0.9954\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 121s 139ms/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 118s 137ms/sample - loss: 0.0023 - acc: 0.9988\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 119s 138ms/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 5.5211e-04 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 123s 142ms/sample - loss: 2.4907e-04 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 122s 141ms/sample - loss: 2.3983e-04 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 1.5003e-04 - acc: 1.0000\n",
      "Score for fold 8: loss of 1.081629991531372; acc of 71.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 111s 128ms/sample - loss: 3.2381 - acc: 0.3545\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 120s 139ms/sample - loss: 0.6639 - acc: 0.7760\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 121s 140ms/sample - loss: 0.3318 - acc: 0.8845\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 117s 135ms/sample - loss: 0.1246 - acc: 0.9630\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 108s 125ms/sample - loss: 0.0820 - acc: 0.9711\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 115s 133ms/sample - loss: 0.0423 - acc: 0.9873\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 118s 136ms/sample - loss: 0.0188 - acc: 0.9977\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 110s 127ms/sample - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 132s 152ms/sample - loss: 0.0064 - acc: 0.9965\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 157s 182ms/sample - loss: 0.0036 - acc: 0.9988\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 8.7586e-04 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 0.0040 - acc: 0.9988\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 158s 182ms/sample - loss: 0.0116 - acc: 0.9954\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 158s 182ms/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 0.0337 - acc: 0.9873\n",
      "Score for fold 9: loss of 0.24768421053886414; acc of 86.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/15\n",
      "866/866 [==============================] - 160s 184ms/sample - loss: 2.9163 - acc: 0.4192\n",
      "Epoch 2/15\n",
      "866/866 [==============================] - 157s 182ms/sample - loss: 0.5174 - acc: 0.8314\n",
      "Epoch 3/15\n",
      "866/866 [==============================] - 158s 182ms/sample - loss: 0.1970 - acc: 0.9388\n",
      "Epoch 4/15\n",
      "866/866 [==============================] - 156s 180ms/sample - loss: 0.0883 - acc: 0.9711\n",
      "Epoch 5/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 0.0528 - acc: 0.9827\n",
      "Epoch 6/15\n",
      "866/866 [==============================] - 158s 182ms/sample - loss: 0.0368 - acc: 0.9873\n",
      "Epoch 7/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 0.0205 - acc: 0.9931\n",
      "Epoch 8/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 0.0256 - acc: 0.9885\n",
      "Epoch 9/15\n",
      "866/866 [==============================] - 158s 182ms/sample - loss: 0.0191 - acc: 0.9965\n",
      "Epoch 10/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 11/15\n",
      "866/866 [==============================] - 157s 182ms/sample - loss: 9.9387e-04 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "866/866 [==============================] - 156s 181ms/sample - loss: 3.7488e-04 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "866/866 [==============================] - 157s 181ms/sample - loss: 2.3404e-04 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "866/866 [==============================] - 156s 181ms/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "866/866 [==============================] - 157s 182ms/sample - loss: 3.2055e-04 - acc: 1.0000\n",
      "Score for fold 10: loss of 0.4988597333431244; acc of 85.41666865348816%\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(inputs, targets):\n",
    "    #Building own model on top of trained network\n",
    "    x = Conv2D(500, (1,1), activation = 'relu')(last)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1028, activation = 'relu')(x)\n",
    "    x = Dropout(rate = 0.1)(x)\n",
    "    x = Dense(512, activation = 'relu')(x)\n",
    "    x = Dropout(rate = 0.1)(x)\n",
    "    x = Dense(4, activation = 'softmax')(x)\n",
    "\n",
    "    #Compiling model\n",
    "    model = Model(inputs = pre_trained_model.input, outputs = x, name = 'Predict')\n",
    "    opt1 = Adam(learning_rate = 0.001)\n",
    "    opt2 = RMSprop(learning_rate = 0.001)\n",
    "    model.compile(optimizer = opt1 , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=no_epochs,\n",
    "              verbose=verbosity,\n",
    "              #validation_data = test_set,\n",
    "              #validation_steps = 5          \n",
    "              )\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5871885579425035 - Accuracy: 81.44329786300659%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.38549215523202407 - Accuracy: 90.7216489315033%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 3.5370729764302573 - Accuracy: 61.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2696586698293686 - Accuracy: 91.66666865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.6566379070281982 - Accuracy: 61.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.581248164176941 - Accuracy: 67.70833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 2.7730469703674316 - Accuracy: 50.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.081629991531372 - Accuracy: 71.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.24768421053886414 - Accuracy: 86.45833134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.4988597333431244 - Accuracy: 85.41666865348816%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 74.82066094875336 (+- 13.640493140667665)\n",
      "> Loss: 1.2618519336420086\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
