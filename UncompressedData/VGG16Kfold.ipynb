{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING THE BRAIN TUMOR MODEL WITH VGG16 PRETRAINED MODEL\n",
    "\n",
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import PIL\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #allows us to augment and process data fed into a CNN\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os,cv2,random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #A tool that allows us to augment and process data to be fed into a CNN\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_inception = \"D:/bcd/data/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "local_weights_file = path_inception\n",
    "pre_trained_model = VGG16(input_shape = (240,240,3),\n",
    "              include_top = False,\n",
    "              weights = None)\n",
    "pre_trained_model.load_weights(local_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last layer output shape :  (?, 15, 15, 512)\n"
     ]
    }
   ],
   "source": [
    "#Locking the weights and parameters of pretrained model\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Getting desired layer output\n",
    "last_layer = pre_trained_model.get_layer('block5_conv2')\n",
    "last = last_layer.output\n",
    "print('Last layer output shape : ', last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "# Model configuration\n",
    "batch_size = 50\n",
    "img_size, img_num_channels = 240, 3\n",
    "loss_function = categorical_crossentropy\n",
    "no_classes = 4\n",
    "no_epochs = 20\n",
    "optimizer = Adam()\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='D:/bcd/data/trainingImages'\n",
    "list_folder=os.listdir(path = data_path)\n",
    "data=[]\n",
    "im_size=240    \n",
    "for i in list_folder:\n",
    "    new_path=os.path.join(data_path,i) \n",
    "    pic_list=os.listdir(new_path)                                               \n",
    "    for img in pic_list:\n",
    "        pic=os.path.join(new_path,img)   \n",
    "        arr=cv2.imread(pic)    \n",
    "        data.append([arr,list_folder.index(i)])    \n",
    "        \n",
    "random.shuffle(data)  \n",
    "x_train,y_train=[],[]\n",
    "for i,j in data:\n",
    "    x_train.append(i)\n",
    "    y_train.append(j)\n",
    "x_train=np.array(x_train).reshape(-1,im_size,im_size,3)\n",
    "y_train=np.array(y_train).reshape(-1,1)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "X_train = x_train/255\n",
    "y_train = y_train.toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train,y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse numbers as floats\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = [] #stores accuracy\n",
    "loss_per_fold = [] #stores losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 962 images belonging to 4 classes.\n",
      "Found 500 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#class for training set\n",
    "train_data = ImageDataGenerator(rescale = 1./255, #pixel values rescaled so that it stays between 0 and 1.\n",
    "                                shear_range = 0.2, #this and the next 4 steps are used to augment our data by shearing it, flipping and zooming it to produce more examples per image.\n",
    "                                zoom_range = 0.2,  #Data augmentation allows us to prevent overfitting of data into the training set.\n",
    "                                horizontal_flip = True,\n",
    "                                vertical_flip = True,\n",
    "                                rotation_range = 40,\n",
    "                                width_shift_range = 0.2,\n",
    "                                height_shift_range = 0.2,\n",
    "                                fill_mode = 'nearest'\n",
    "                                )\n",
    "test_data = ImageDataGenerator(rescale = 1/.255)\n",
    "\n",
    "train_set = train_data.flow_from_directory('D:/bcd/data/trainingImages', #Image path\n",
    "                                           target_size = (240,240), #The shape which we want to input our images in our model\n",
    "                                           batch_size = 20,         #The batch size in mini batch gradient descent\n",
    "                                           class_mode = 'categorical')   \n",
    "\n",
    "\n",
    "test_set = test_data.flow_from_directory('D:/bcd/data/validationImages',\n",
    "                                         target_size = (240,240),\n",
    "                                         batch_size = 50,\n",
    "                                         class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds=5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/20\n",
      "769/769 [==============================] - 187s 243ms/sample - loss: 29.4719 - acc: 0.3108\n",
      "Epoch 2/20\n",
      "769/769 [==============================] - 170s 222ms/sample - loss: 9.7198 - acc: 0.7074\n",
      "Epoch 3/20\n",
      "769/769 [==============================] - 171s 222ms/sample - loss: 5.8251 - acc: 0.9038\n",
      "Epoch 4/20\n",
      "769/769 [==============================] - 170s 220ms/sample - loss: 3.6751 - acc: 0.9246\n",
      "Epoch 5/20\n",
      "769/769 [==============================] - 172s 223ms/sample - loss: 2.4068 - acc: 0.9623\n",
      "Epoch 6/20\n",
      "769/769 [==============================] - 170s 221ms/sample - loss: 1.6648 - acc: 0.9597\n",
      "Epoch 7/20\n",
      "769/769 [==============================] - 172s 224ms/sample - loss: 1.1561 - acc: 0.9870\n",
      "Epoch 8/20\n",
      "769/769 [==============================] - 171s 222ms/sample - loss: 0.8759 - acc: 0.9792\n",
      "Epoch 9/20\n",
      "769/769 [==============================] - 174s 226ms/sample - loss: 0.6494 - acc: 0.9896\n",
      "Epoch 10/20\n",
      "769/769 [==============================] - 173s 225ms/sample - loss: 0.4924 - acc: 0.9948\n",
      "Epoch 11/20\n",
      "769/769 [==============================] - 170s 221ms/sample - loss: 0.3620 - acc: 0.9987\n",
      "Epoch 12/20\n",
      "769/769 [==============================] - 176s 229ms/sample - loss: 0.3521 - acc: 0.9753\n",
      "Epoch 13/20\n",
      "769/769 [==============================] - 173s 225ms/sample - loss: 0.3086 - acc: 0.9857\n",
      "Epoch 14/20\n",
      "769/769 [==============================] - 196s 254ms/sample - loss: 0.2222 - acc: 0.9974\n",
      "Epoch 15/20\n",
      "769/769 [==============================] - 190s 248ms/sample - loss: 0.1644 - acc: 0.9974\n",
      "Epoch 16/20\n",
      "769/769 [==============================] - 172s 224ms/sample - loss: 0.1474 - acc: 0.9948\n",
      "Epoch 17/20\n",
      "769/769 [==============================] - 181s 235ms/sample - loss: 0.1273 - acc: 0.9961\n",
      "Epoch 18/20\n",
      "769/769 [==============================] - 180s 234ms/sample - loss: 0.1350 - acc: 0.9883\n",
      "Epoch 19/20\n",
      "769/769 [==============================] - 191s 248ms/sample - loss: 0.1079 - acc: 0.9974\n",
      "Epoch 20/20\n",
      "769/769 [==============================] - 173s 225ms/sample - loss: 0.1040 - acc: 0.9896\n",
      "Score for fold 1: loss of 0.23696435123218773; acc of 93.26424598693848%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/20\n",
      "769/769 [==============================] - 385s 501ms/sample - loss: 28.2196 - acc: 0.3381\n",
      "Epoch 2/20\n",
      "769/769 [==============================] - 177s 230ms/sample - loss: 9.2623 - acc: 0.7724\n",
      "Epoch 3/20\n",
      "769/769 [==============================] - 1138s 1s/sample - loss: 5.4930 - acc: 0.8999\n",
      "Epoch 4/20\n",
      "769/769 [==============================] - 176s 229ms/sample - loss: 3.3818 - acc: 0.9571\n",
      "Epoch 5/20\n",
      "769/769 [==============================] - 174s 226ms/sample - loss: 2.2151 - acc: 0.9727\n",
      "Epoch 6/20\n",
      "769/769 [==============================] - 176s 228ms/sample - loss: 1.5055 - acc: 0.9909\n",
      "Epoch 7/20\n",
      "769/769 [==============================] - 173s 225ms/sample - loss: 1.0907 - acc: 0.9831\n",
      "Epoch 8/20\n",
      "769/769 [==============================] - 180s 234ms/sample - loss: 0.8437 - acc: 0.9792\n",
      "Epoch 9/20\n",
      "769/769 [==============================] - 177s 230ms/sample - loss: 0.6352 - acc: 0.9844\n",
      "Epoch 10/20\n",
      "769/769 [==============================] - 184s 239ms/sample - loss: 0.4740 - acc: 0.9935\n",
      "Epoch 11/20\n",
      "769/769 [==============================] - 177s 230ms/sample - loss: 0.3501 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "769/769 [==============================] - 191s 248ms/sample - loss: 0.2654 - acc: 0.9987\n",
      "Epoch 13/20\n",
      "769/769 [==============================] - 174s 227ms/sample - loss: 0.2140 - acc: 0.9987\n",
      "Epoch 14/20\n",
      "769/769 [==============================] - 182s 236ms/sample - loss: 0.2063 - acc: 0.9922\n",
      "Epoch 15/20\n",
      "769/769 [==============================] - 180s 233ms/sample - loss: 0.1766 - acc: 0.9961\n",
      "Epoch 16/20\n",
      "769/769 [==============================] - 178s 232ms/sample - loss: 0.1323 - acc: 0.9987\n",
      "Epoch 17/20\n",
      "769/769 [==============================] - 181s 236ms/sample - loss: 0.1150 - acc: 0.9987\n",
      "Epoch 18/20\n",
      "769/769 [==============================] - 173s 225ms/sample - loss: 0.1131 - acc: 0.9922\n",
      "Epoch 19/20\n",
      "769/769 [==============================] - 181s 235ms/sample - loss: 0.1271 - acc: 0.9909\n",
      "Epoch 20/20\n",
      "769/769 [==============================] - 200s 261ms/sample - loss: 0.1997 - acc: 0.9779\n",
      "Score for fold 2: loss of 0.5871401577415861; acc of 88.08290362358093%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/20\n",
      "770/770 [==============================] - 347s 450ms/sample - loss: 26.3101 - acc: 0.3364\n",
      "Epoch 2/20\n",
      "770/770 [==============================] - 263s 342ms/sample - loss: 8.9166 - acc: 0.6948\n",
      "Epoch 3/20\n",
      "770/770 [==============================] - 273s 355ms/sample - loss: 5.1365 - acc: 0.8870\n",
      "Epoch 4/20\n",
      "770/770 [==============================] - 270s 351ms/sample - loss: 3.1155 - acc: 0.9390\n",
      "Epoch 5/20\n",
      "770/770 [==============================] - 277s 360ms/sample - loss: 2.0061 - acc: 0.9597\n",
      "Epoch 6/20\n",
      "770/770 [==============================] - 276s 358ms/sample - loss: 1.3722 - acc: 0.9714\n",
      "Epoch 7/20\n",
      "770/770 [==============================] - 270s 351ms/sample - loss: 0.9436 - acc: 0.9857\n",
      "Epoch 8/20\n",
      "770/770 [==============================] - 286s 371ms/sample - loss: 0.7206 - acc: 0.9805\n",
      "Epoch 9/20\n",
      "770/770 [==============================] - 284s 368ms/sample - loss: 0.5198 - acc: 0.9974\n",
      "Epoch 10/20\n",
      "770/770 [==============================] - 267s 346ms/sample - loss: 0.3799 - acc: 0.9961\n",
      "Epoch 11/20\n",
      "770/770 [==============================] - 290s 376ms/sample - loss: 0.2837 - acc: 0.9974\n",
      "Epoch 12/20\n",
      "770/770 [==============================] - 239s 311ms/sample - loss: 0.2224 - acc: 0.9961\n",
      "Epoch 13/20\n",
      "770/770 [==============================] - 234s 304ms/sample - loss: 0.1876 - acc: 0.9935\n",
      "Epoch 14/20\n",
      "770/770 [==============================] - 217s 281ms/sample - loss: 0.1902 - acc: 0.9883\n",
      "Epoch 15/20\n",
      "770/770 [==============================] - 233s 302ms/sample - loss: 0.2441 - acc: 0.9727\n",
      "Epoch 16/20\n",
      "770/770 [==============================] - 214s 277ms/sample - loss: 0.1998 - acc: 0.9935\n",
      "Epoch 17/20\n",
      "770/770 [==============================] - 230s 299ms/sample - loss: 0.1364 - acc: 0.9961\n",
      "Epoch 18/20\n",
      "770/770 [==============================] - 211s 274ms/sample - loss: 0.0929 - acc: 0.9974\n",
      "Epoch 19/20\n",
      "770/770 [==============================] - 235s 305ms/sample - loss: 0.0824 - acc: 0.9987\n",
      "Epoch 20/20\n",
      "770/770 [==============================] - 215s 280ms/sample - loss: 0.0658 - acc: 1.0000\n",
      "Score for fold 3: loss of 0.22696564656992754; acc of 94.79166865348816%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/20\n",
      "770/770 [==============================] - 237s 308ms/sample - loss: 27.8812 - acc: 0.3623\n",
      "Epoch 2/20\n",
      "770/770 [==============================] - 228s 296ms/sample - loss: 9.0710 - acc: 0.7416\n",
      "Epoch 3/20\n",
      "770/770 [==============================] - 212s 275ms/sample - loss: 5.3171 - acc: 0.8987\n",
      "Epoch 4/20\n",
      "770/770 [==============================] - 223s 290ms/sample - loss: 3.2764 - acc: 0.9468\n",
      "Epoch 5/20\n",
      "770/770 [==============================] - 215s 279ms/sample - loss: 2.1392 - acc: 0.9662\n",
      "Epoch 6/20\n",
      "770/770 [==============================] - 213s 277ms/sample - loss: 1.4546 - acc: 0.9779\n",
      "Epoch 7/20\n",
      "770/770 [==============================] - 209s 271ms/sample - loss: 1.0301 - acc: 0.9870\n",
      "Epoch 8/20\n",
      "770/770 [==============================] - 232s 302ms/sample - loss: 0.7715 - acc: 0.9844\n",
      "Epoch 9/20\n",
      "770/770 [==============================] - 228s 297ms/sample - loss: 0.5666 - acc: 0.9948\n",
      "Epoch 10/20\n",
      "770/770 [==============================] - 224s 291ms/sample - loss: 0.4138 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "770/770 [==============================] - 234s 303ms/sample - loss: 0.3357 - acc: 0.9870\n",
      "Epoch 12/20\n",
      "770/770 [==============================] - 234s 303ms/sample - loss: 0.2802 - acc: 0.9922\n",
      "Epoch 13/20\n",
      "770/770 [==============================] - 233s 303ms/sample - loss: 0.2748 - acc: 0.9753\n",
      "Epoch 14/20\n",
      "770/770 [==============================] - 234s 304ms/sample - loss: 0.2774 - acc: 0.9805\n",
      "Epoch 15/20\n",
      "770/770 [==============================] - 231s 301ms/sample - loss: 0.2194 - acc: 0.9857\n",
      "Epoch 16/20\n",
      "770/770 [==============================] - 234s 304ms/sample - loss: 0.1795 - acc: 0.9935\n",
      "Epoch 17/20\n",
      "770/770 [==============================] - 233s 302ms/sample - loss: 0.1712 - acc: 0.9896\n",
      "Epoch 18/20\n",
      "770/770 [==============================] - 233s 303ms/sample - loss: 0.1407 - acc: 0.9961\n",
      "Epoch 19/20\n",
      "770/770 [==============================] - 233s 302ms/sample - loss: 0.1191 - acc: 0.9948\n",
      "Epoch 20/20\n",
      "770/770 [==============================] - 235s 305ms/sample - loss: 0.0918 - acc: 0.9961\n",
      "Score for fold 4: loss of 0.24568949888149896; acc of 94.79166865348816%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/20\n",
      "770/770 [==============================] - 249s 323ms/sample - loss: 28.4788 - acc: 0.3104\n",
      "Epoch 2/20\n",
      "770/770 [==============================] - 259s 336ms/sample - loss: 9.8724 - acc: 0.6987\n",
      "Epoch 3/20\n",
      "770/770 [==============================] - 233s 303ms/sample - loss: 5.8001 - acc: 0.8662\n",
      "Epoch 4/20\n",
      "770/770 [==============================] - 235s 305ms/sample - loss: 3.4488 - acc: 0.9468\n",
      "Epoch 5/20\n",
      "770/770 [==============================] - 235s 305ms/sample - loss: 2.1785 - acc: 0.9740\n",
      "Epoch 6/20\n",
      "770/770 [==============================] - 232s 302ms/sample - loss: 1.4386 - acc: 0.9857\n",
      "Epoch 7/20\n",
      "770/770 [==============================] - 233s 302ms/sample - loss: 1.0294 - acc: 0.9818\n",
      "Epoch 8/20\n",
      "770/770 [==============================] - 234s 303ms/sample - loss: 0.7376 - acc: 0.9896\n",
      "Epoch 9/20\n",
      "770/770 [==============================] - 234s 304ms/sample - loss: 0.5421 - acc: 0.9935\n",
      "Epoch 10/20\n",
      "770/770 [==============================] - 235s 305ms/sample - loss: 0.4133 - acc: 0.9948\n",
      "Epoch 11/20\n",
      "770/770 [==============================] - 233s 302ms/sample - loss: 0.3388 - acc: 0.9870\n",
      "Epoch 12/20\n",
      "770/770 [==============================] - 232s 301ms/sample - loss: 0.2833 - acc: 0.9896\n",
      "Epoch 13/20\n",
      "770/770 [==============================] - 234s 303ms/sample - loss: 0.2422 - acc: 0.9896\n",
      "Epoch 14/20\n",
      "770/770 [==============================] - 234s 303ms/sample - loss: 0.1988 - acc: 0.9935\n",
      "Epoch 15/20\n",
      "770/770 [==============================] - 234s 304ms/sample - loss: 0.1805 - acc: 0.9896\n",
      "Epoch 16/20\n",
      "770/770 [==============================] - 239s 310ms/sample - loss: 0.2186 - acc: 0.9753\n",
      "Epoch 17/20\n",
      "770/770 [==============================] - 239s 310ms/sample - loss: 0.2097 - acc: 0.9883\n",
      "Epoch 18/20\n",
      "770/770 [==============================] - 238s 309ms/sample - loss: 0.1639 - acc: 0.9948\n",
      "Epoch 19/20\n",
      "770/770 [==============================] - 240s 312ms/sample - loss: 0.1119 - acc: 0.9987\n",
      "Epoch 20/20\n",
      "770/770 [==============================] - 242s 314ms/sample - loss: 0.0744 - acc: 1.0000\n",
      "Score for fold 5: loss of 0.20915491630633673; acc of 95.83333134651184%\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(inputs, targets):\n",
    "    #Building own model on top of trained network\n",
    "    x = Conv2D(500, (1,1), activation = 'relu')(last)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1028, activation = 'relu',kernel_regularizer='l2')(x) #default of l2=0.01\n",
    "    x = Dropout(rate = 0.15)(x)\n",
    "    x = Dense(512, activation = 'relu')(x)\n",
    "    x = Dropout(rate = 0.2)(x)\n",
    "    x = Dense(4, activation = 'softmax')(x)\n",
    "\n",
    "    #Compiling model\n",
    "    model = Model(inputs = pre_trained_model.input, outputs = x, name = 'Predict')\n",
    "    opt1 = Adam(learning_rate = 0.001)\n",
    "    opt2 = RMSprop(learning_rate = 0.001)\n",
    "    model.compile(optimizer = opt1 , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.23696435123218773 - Accuracy: 93.26424598693848%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5871401577415861 - Accuracy: 88.08290362358093%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.22696564656992754 - Accuracy: 94.79166865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24568949888149896 - Accuracy: 94.79166865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.20915491630633673 - Accuracy: 95.83333134651184%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 93.35276365280151 (+- 2.7594699269906178)\n",
      "> Loss: 0.30118291414630743\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
